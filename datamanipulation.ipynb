{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data:\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "ii = 0\n",
    "for d in [r\"data_\\training\",r\"data_\\training\"]:\n",
    "    for dd in os.listdir(d):\n",
    "        if 'images' in dd:\n",
    "            for img in os.listdir(os.path.join(d,dd)):\n",
    "                # Generate a random float between 0 and 1\n",
    "                random_number = random.random()\n",
    "                if random_number < test_size:\n",
    "                    shutil.copy(os.path.join(d,dd,img),os.path.join('data','validation','images',img))\n",
    "                    shutil.copy(os.path.join(d,dd.replace('images','labels'),img.replace('.jpg','.txt')),os.path.join('data','validation','labels',img.replace('.jpg','.txt')))\n",
    "                else:\n",
    "                    shutil.copy(os.path.join(d,dd,img),os.path.join('data','training','images',img))\n",
    "                    shutil.copy(os.path.join(d,dd.replace('images','labels'),img.replace('.jpg','.txt')),os.path.join('data','training','labels',img.replace('.jpg','.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [r\"data\\training\\images\",r\"data\\validation\\images\"]:\n",
    "    for img in os.listdir(dir_):\n",
    "        if 'over' in img.lower() and dir_ == r\"data\\training\\images\":\n",
    "            shutil.move(os.path.join(dir_,img),os.path.join(r\"data\\validation\\images\",img))\n",
    "        elif 'under' in img.lower() and dir_ == r\"data\\validation\\images\":\n",
    "            shutil.move(os.path.join(dir_,img),os.path.join(r\"data\\training\\images\",img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [r\"data\\training\\labels\",r\"data\\validation\\labels\"]:\n",
    "    for lbl in os.listdir(dir_):\n",
    "        if 'over' in lbl.lower() and dir_ == r\"data\\training\\labels\":\n",
    "            shutil.move(os.path.join(dir_,lbl),os.path.join(r\"data\\validation\\labels\",lbl))\n",
    "        elif 'under' in lbl.lower() and dir_ == r\"data\\validation\\labels\":\n",
    "            shutil.move(os.path.join(dir_,lbl),os.path.join(r\"data\\training\\labels\",lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\"):\n",
    "    if img.replace('.jpg','.txt') not in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\labels\"):\n",
    "        os.remove(os.path.join(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\",img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\"):\n",
    "    val_img = '-'+ img\n",
    "    if val_img in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\validation\\images\"):\n",
    "        os.remove(os.path.join(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\",img))\n",
    "        os.remove(os.path.join(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\labels\",img.replace('jpg','txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsdir = r'data/validation/labels/'\n",
    "\n",
    "for labelfile in os.listdir(labelsdir):\n",
    "    if 'over'in labelfile.lower() or 'under' in labelfile.lower() or 'human' in labelfile.lower():\n",
    "        # Open the file in read mode and read its contents\n",
    "        with open(os.path.join(labelsdir,labelfile), 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Modify the desired text\n",
    "        modified_text = text.replace('4 ', '9 ')\n",
    "        modified_text = modified_text.replace('1 ', '4 ')\n",
    "        modified_text = modified_text.replace('2 ', '12 ')\n",
    "        modified_text = modified_text.replace('3 ', '1 ')\n",
    "        modified_text = modified_text.replace('7 ', '13 ')\n",
    "        modified_text = modified_text.replace('5 ', '7 ')\n",
    "        modified_text = modified_text.replace('6 ', '11 ')\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # Open the file in write mode and overwrite the original contents\n",
    "        with open(os.path.join(labelsdir,labelfile),'w') as file:\n",
    "            file.write(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted HumanPowered-00fdac90-e992-4535-98a9-0f55551d7fe4.xml to YOLO format.\n",
      "Converted HumanPowered-027d7ce7-f99b-4948-a0a3-7b101d408fd0.xml to YOLO format.\n",
      "Converted HumanPowered-02d356de-b43a-465d-9c2d-1edd5cf99efe.xml to YOLO format.\n",
      "Converted HumanPowered-053a90bc-47c3-4f3c-841d-4c1612687054.xml to YOLO format.\n",
      "Converted HumanPowered-05bb50dc-ce3b-47bb-a6b3-3a1ba2a0c9ed.xml to YOLO format.\n",
      "Converted HumanPowered-09c82b45-b717-4ce5-903b-3bba34852fff.xml to YOLO format.\n",
      "Converted HumanPowered-0a5e8b27-87c6-4633-87f3-d98e054c2780.xml to YOLO format.\n",
      "Converted HumanPowered-0c2589de-52e6-4cd3-a707-df12f10086cf.xml to YOLO format.\n",
      "Converted HumanPowered-0d73e95f-7f7c-4c7f-a3db-90acc0dd0bd5.xml to YOLO format.\n",
      "Converted HumanPowered-11bdf9dd-07c2-4fd6-9861-f4d65043aeb3.xml to YOLO format.\n",
      "Converted HumanPowered-13b49798-b1f9-4e91-a419-57b64adba3eb.xml to YOLO format.\n",
      "Converted HumanPowered-14b4e542-ef60-4b51-95d6-9009735cd197.xml to YOLO format.\n",
      "Converted HumanPowered-16199f3e-895e-448b-add1-b38e37fb2f93.xml to YOLO format.\n",
      "Converted HumanPowered-16e9ab51-7842-44b2-a691-9e106430f238.xml to YOLO format.\n",
      "Converted HumanPowered-17da6501-95e4-4571-ae32-c96ee3556663.xml to YOLO format.\n",
      "Converted HumanPowered-1aa3cf2e-da7c-4e5c-8159-cb5d879b0cea.xml to YOLO format.\n",
      "Converted HumanPowered-1ba2dfe9-3982-4635-865e-13462f41bf16.xml to YOLO format.\n",
      "Converted HumanPowered-1d00b86c-f921-413a-9960-5f89177eb25d.xml to YOLO format.\n",
      "Converted HumanPowered-1d4a450d-4d29-416d-82e6-fb147c3be129.xml to YOLO format.\n",
      "Converted HumanPowered-1e99cdef-e511-4127-802e-ab8aaa9be416.xml to YOLO format.\n",
      "Converted HumanPowered-1f0a9347-9f94-46cf-a112-532bbe5f4e60.xml to YOLO format.\n",
      "Converted HumanPowered-1fd702bf-5a21-4c6a-b056-fb660211871c.xml to YOLO format.\n",
      "Converted HumanPowered-1fff973d-9f4e-4551-aa34-f15dedf1f92a.xml to YOLO format.\n",
      "Converted HumanPowered-20f39fee-5f85-46da-8d09-b1cc82e64adb.xml to YOLO format.\n",
      "Converted HumanPowered-2649b5f2-414a-4323-9638-4520802f884c.xml to YOLO format.\n",
      "Converted HumanPowered-27caa33e-1042-4eae-b2ba-c8b327f0df96.xml to YOLO format.\n",
      "Converted HumanPowered-2807c418-068f-4aa7-8eb4-c1f1c2610ea1.xml to YOLO format.\n",
      "Converted HumanPowered-29db248f-06de-4ffb-859b-8c8e957352cb.xml to YOLO format.\n",
      "Converted HumanPowered-2e9ba377-b587-4ac7-ae18-5e59f141a517.xml to YOLO format.\n",
      "Converted HumanPowered-2fde4599-39db-4caf-892b-f7acd6100535.xml to YOLO format.\n",
      "Converted HumanPowered-34419cac-68ca-4b79-aafd-8e117c47fad3.xml to YOLO format.\n",
      "Converted HumanPowered-364bf4eb-d5ba-402a-a020-f6a8946ed845.xml to YOLO format.\n",
      "Converted HumanPowered-36db6587-a2ab-443c-8b89-babe7532abc4.xml to YOLO format.\n",
      "Converted HumanPowered-3792581d-62b7-481a-a80b-66168ba68c3e.xml to YOLO format.\n",
      "Converted HumanPowered-3ab28180-06cf-4feb-bf0f-a06f0b484dcc.xml to YOLO format.\n",
      "Converted HumanPowered-3b1f3ebd-e958-4d74-9b6d-b7140855fe04.xml to YOLO format.\n",
      "Converted HumanPowered-3bdcf97a-7d2e-47eb-afbd-34625297d786.xml to YOLO format.\n",
      "Converted HumanPowered-3f202a3e-4419-4f41-8fbc-fbff163abc16.xml to YOLO format.\n",
      "Converted HumanPowered-3f330ef1-ffce-45dd-8984-f68dbc04fd9b.xml to YOLO format.\n",
      "Converted HumanPowered-3f339c58-adb7-4b35-bf55-f79814ea3087.xml to YOLO format.\n",
      "Converted HumanPowered-44e35e50-b718-437c-9288-c7d75760e28d.xml to YOLO format.\n",
      "Converted HumanPowered-480d23ff-6236-421e-b923-68fb0fb0aa54.xml to YOLO format.\n",
      "Converted HumanPowered-48c0f4bc-8ceb-4523-8f4a-fabec1dd1012.xml to YOLO format.\n",
      "Converted HumanPowered-4aa77ed7-afec-425e-af7d-3b8870c1683c.xml to YOLO format.\n",
      "Converted HumanPowered-4cf621e2-bf31-483a-9b2b-f3aa56efb087.xml to YOLO format.\n",
      "Converted HumanPowered-4d4c798e-935b-4b85-a05a-240b6825a765.xml to YOLO format.\n",
      "Converted HumanPowered-4f981c10-2395-4d08-8909-9ee5a2556e0b.xml to YOLO format.\n",
      "Converted HumanPowered-4fc3ea97-1016-4378-b898-cd4284ec6cfb.xml to YOLO format.\n",
      "Converted HumanPowered-4fceb65e-dba3-49b3-be0a-b3c6517f0887.xml to YOLO format.\n",
      "Converted HumanPowered-52db48a6-f44a-44c2-bee0-1ddce3297b86.xml to YOLO format.\n",
      "Converted HumanPowered-53ad1222-f749-4b58-82e4-344906af69f3.xml to YOLO format.\n",
      "Converted HumanPowered-546eec18-5486-453c-a931-d5a75baf9d26.xml to YOLO format.\n",
      "Converted HumanPowered-5a7d374c-51ee-4a97-8f02-162300006175.xml to YOLO format.\n",
      "Converted HumanPowered-5b8abd26-184f-4a8d-8e16-124f82bb3333.xml to YOLO format.\n",
      "Converted HumanPowered-5ebea1c7-598a-482f-91c2-3dd522d93647.xml to YOLO format.\n",
      "Converted HumanPowered-5fb6d07f-f1d0-493e-ab22-269d12d716c4.xml to YOLO format.\n",
      "Converted HumanPowered-606f7262-407c-40e7-ab0a-ee19f4a67f97.xml to YOLO format.\n",
      "Converted HumanPowered-613479bb-5aa0-40ea-81c0-0423b509b6bd.xml to YOLO format.\n",
      "Converted HumanPowered-6350a5e7-e69c-47a4-b728-e586cb947384.xml to YOLO format.\n",
      "Converted HumanPowered-647a4ec4-b0b7-41e9-bbaf-dd687e12bc5c.xml to YOLO format.\n",
      "Converted HumanPowered-64c25c9c-6016-4fdb-ab27-8346d7fc5f2e.xml to YOLO format.\n",
      "Converted HumanPowered-6548c1ef-9fc9-4dcf-90f3-9b97b630bd1d.xml to YOLO format.\n",
      "Converted HumanPowered-6630642f-0442-4444-97e6-f411b7da9c22.xml to YOLO format.\n",
      "Converted HumanPowered-67f81ade-9ca4-4063-85e7-e63cfc1916ee.xml to YOLO format.\n",
      "Converted HumanPowered-69431128-2ffb-4292-bff6-8ccfd63c0fb9.xml to YOLO format.\n",
      "Converted HumanPowered-69707bd9-7cde-4d8c-befa-08d43f6ead6b.xml to YOLO format.\n",
      "Converted HumanPowered-6a998f5e-b607-42ce-8fe7-85c691b4665e.xml to YOLO format.\n",
      "Converted HumanPowered-6c7bea3f-f6b3-489b-8e78-39701091d9d5.xml to YOLO format.\n",
      "Converted HumanPowered-6dead394-ca11-47f7-87d6-16ce13dc31e8.xml to YOLO format.\n",
      "Converted HumanPowered-6f0db280-c80d-47ca-9dfe-53b9b3301545.xml to YOLO format.\n",
      "Converted HumanPowered-6f102e37-0f91-4ae2-bce5-04d85fac8a84.xml to YOLO format.\n",
      "Converted HumanPowered-761d22e0-6c44-42de-b71e-3d1defc823b7.xml to YOLO format.\n",
      "Converted HumanPowered-76e53714-f634-4216-9da9-cee3171d0c01.xml to YOLO format.\n",
      "Converted HumanPowered-78e21fd5-da09-4ec6-bdbe-9071ff689f8d.xml to YOLO format.\n",
      "Converted HumanPowered-79a490e4-bf73-4e8a-b15f-3dda90375b9a.xml to YOLO format.\n",
      "Converted HumanPowered-79ef7ea4-364f-4cd3-8005-7afa0a2034da.xml to YOLO format.\n",
      "Converted HumanPowered-7ac2abfe-c29b-4ee8-b750-c0728fc74787.xml to YOLO format.\n",
      "Converted HumanPowered-7c5f192e-7d22-431e-ac23-e6201a691775.xml to YOLO format.\n",
      "Converted HumanPowered-7cecc898-6354-4b88-b2dd-26a2d8513fc4.xml to YOLO format.\n",
      "Converted HumanPowered-7f37f1bb-1fcc-4dce-8c7d-fd270270d048.xml to YOLO format.\n",
      "Converted HumanPowered-7f51b0eb-86c6-4858-9e16-0f512bda752d.xml to YOLO format.\n",
      "Converted HumanPowered-7f6992d4-4a39-4508-86f4-8b3a00240b6e.xml to YOLO format.\n",
      "Converted HumanPowered-81c5fbbf-6e92-4b65-84db-f09c7b98dd14.xml to YOLO format.\n",
      "Converted HumanPowered-87a9caf4-fdd9-433c-93c1-6941870151d9.xml to YOLO format.\n",
      "Converted HumanPowered-8eabba4b-029a-445d-b4fc-8dfbf664eb93.xml to YOLO format.\n",
      "Converted HumanPowered-8edca716-a533-4bed-8dc2-036737db902c.xml to YOLO format.\n",
      "Converted HumanPowered-8f3980aa-74b2-437e-b17c-d412afcdf306.xml to YOLO format.\n",
      "Converted HumanPowered-90f8b4b9-88a8-492f-bf25-c3affa38a408.xml to YOLO format.\n",
      "Converted HumanPowered-93e98e80-2fcb-48d5-a4dd-ea2d7e1e7cbf.xml to YOLO format.\n",
      "Converted HumanPowered-96a570dd-418a-4300-916f-113f510ad3e8.xml to YOLO format.\n",
      "Converted HumanPowered-9b4f07e2-c954-48c0-8bdc-b3828765bb8c.xml to YOLO format.\n",
      "Converted HumanPowered-9e108372-ff6c-4c31-9569-843396dadcae.xml to YOLO format.\n",
      "Converted HumanPowered-a02acc83-ed82-4a9b-88d0-d6da6a16f91b.xml to YOLO format.\n",
      "Converted HumanPowered-a09a86a1-3ddb-4e86-baca-1c53be5e754b.xml to YOLO format.\n",
      "Converted HumanPowered-a2332114-8744-4ccf-9f89-190d9188fa46.xml to YOLO format.\n",
      "Converted HumanPowered-a345371d-c785-4144-aa4c-987914468ea8.xml to YOLO format.\n",
      "Converted HumanPowered-a8b47366-bc04-4c87-a564-290b93223ca2.xml to YOLO format.\n",
      "Converted HumanPowered-a9067bab-bde4-49b1-b8c3-ec023200ffd4.xml to YOLO format.\n",
      "Converted HumanPowered-a9b4dc97-a8f0-42fe-8fad-ef4046e33be2.xml to YOLO format.\n",
      "Converted HumanPowered-b1c451ec-0515-4400-9c32-cd9aed97bf91.xml to YOLO format.\n",
      "Converted HumanPowered-b3a41cc8-faeb-4d54-a27d-413996edee7c.xml to YOLO format.\n",
      "Converted HumanPowered-b8cd6b9d-8377-44b9-9e69-39e2e842cf8a.xml to YOLO format.\n",
      "Converted HumanPowered-bb6cf815-c50c-4dfa-84ea-e7f911e8812d.xml to YOLO format.\n",
      "Converted HumanPowered-bc491b62-28c7-4e93-bea5-629e50d32926.xml to YOLO format.\n",
      "Converted HumanPowered-bfed682a-f9a2-4ce6-8e03-c9cc1760340b.xml to YOLO format.\n",
      "Converted HumanPowered-c07505f3-bfbe-4a89-a04a-1249954b6ab6.xml to YOLO format.\n",
      "Converted HumanPowered-c7dd7ce0-987f-405f-9184-42db5714cb52.xml to YOLO format.\n",
      "Converted HumanPowered-ce45f4b5-8651-40c1-aab4-5e6bf84e6650.xml to YOLO format.\n",
      "Converted HumanPowered-d0082cc8-50b9-4a6d-aa7e-f228af9fd3f1.xml to YOLO format.\n",
      "Converted HumanPowered-d1afb1fc-71bc-4c64-aada-5f7a4157dec7.xml to YOLO format.\n",
      "Converted HumanPowered-d230df97-7e9d-4e7d-9c66-a0e8c605fd40.xml to YOLO format.\n",
      "Converted HumanPowered-d517a9cc-f10d-47d5-b32a-5998f9376fc0.xml to YOLO format.\n",
      "Converted HumanPowered-d64b325d-17b4-4483-aff7-c9aea1f3d17c.xml to YOLO format.\n",
      "Converted HumanPowered-d8b349ad-ae00-449e-be77-87f2ed90c55a.xml to YOLO format.\n",
      "Converted HumanPowered-dd8b9e5e-e72c-4eac-93d5-e25ee1649a08.xml to YOLO format.\n",
      "Converted HumanPowered-dd937733-8f5a-447b-b1a0-e01f0424047a.xml to YOLO format.\n",
      "Converted HumanPowered-dfd72451-b96d-4d13-99f5-fb720a0a491d.xml to YOLO format.\n",
      "Converted HumanPowered-e0f17027-b6ac-4db0-ad63-7ca590cc42bd.xml to YOLO format.\n",
      "Converted HumanPowered-e1b0ff90-4567-4b8d-8eb8-a10b0090f6d7.xml to YOLO format.\n",
      "Converted HumanPowered-e3542c56-b1e4-4777-822e-6556b53473ea.xml to YOLO format.\n",
      "Converted HumanPowered-e4728fdd-24c9-4d77-9790-8861da341148.xml to YOLO format.\n",
      "Converted HumanPowered-e75c30ce-b0de-4c98-921f-5862dd3c3a6b.xml to YOLO format.\n",
      "Converted HumanPowered-e7cc9f72-36a3-4979-b6b0-f73aa0048c9d.xml to YOLO format.\n",
      "Converted HumanPowered-e85cab24-0d40-49f5-a993-b0f35fb1dab5.xml to YOLO format.\n",
      "Converted HumanPowered-e8e5dce2-50cc-4b95-a518-918363b5e4d3.xml to YOLO format.\n",
      "Converted HumanPowered-eca96716-2aa8-4403-ab94-e893800550c0.xml to YOLO format.\n",
      "Converted HumanPowered-eec0f4a4-b596-43fb-9858-dce96f43f36d.xml to YOLO format.\n",
      "Converted HumanPowered-f380c86b-7b8b-47fe-b56f-f3949c31d76b.xml to YOLO format.\n",
      "Converted HumanPowered-f4892fdc-35e2-42e7-bb21-0ebb0fab56b1.xml to YOLO format.\n",
      "Converted HumanPowered-f53f760b-86dd-4ab9-b554-0b37a49c31cd.xml to YOLO format.\n",
      "Converted HumanPowered-f6fbc1d3-aabf-455f-8d85-5747af7770f1.xml to YOLO format.\n",
      "Converted HumanPowered-f810f9f6-a60b-454a-ac84-a802f100cb1b.xml to YOLO format.\n",
      "Converted HumanPowered-fa4d8721-4bd9-4803-97b7-a6b7af987aaa.xml to YOLO format.\n",
      "Converted HumanPowered-faf4bcb1-cb95-484c-a260-46d47c549d3f.xml to YOLO format.\n",
      "Converted HumanPowered-fb0a5c5d-8dd2-4d5c-a27e-a27c5d743e31.xml to YOLO format.\n",
      "Converted HumanPowered-fbc63ae7-f728-4ec6-ae5a-27022c68ae66.xml to YOLO format.\n",
      "Converted HumanPowered-fc4c2000-abe0-48fd-8bf3-b28095f2e245.xml to YOLO format.\n",
      "Converted HumanPowered-fe8e963f-fee6-4d42-9cc3-22a0f515ed3b.xml to YOLO format.\n",
      "Converted HumanPowered-feb3625f-428f-485e-b53e-629c1a014356.xml to YOLO format.\n",
      "Converted HumanPowered-ff1f2b0e-7b4f-4406-91ba-36b338021e00.xml to YOLO format.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "def convert_pascalvoc_to_yolo(pascal_dir, output_dir, class_list):\n",
    "    for filename in os.listdir(pascal_dir):\n",
    "        if filename.endswith('.xml'):\n",
    "            # Read the XML annotation file\n",
    "            xml_path = os.path.join(pascal_dir, filename)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract image size\n",
    "            width = int(root.find('size/width').text)\n",
    "            height = int(root.find('size/height').text)\n",
    "\n",
    "            # Create the YOLO annotation file\n",
    "            yolo_path = os.path.join(output_dir, filename.replace('.xml', '.txt'))\n",
    "            with open(yolo_path, 'w') as file:\n",
    "                for obj in root.findall('object'):\n",
    "                    # Extract object class and bounding box coordinates\n",
    "                    class_name = obj.find('name').text\n",
    "                    if class_name == 'registration_numver':\n",
    "                        class_name = 'registration_number'\n",
    "                    class_index = class_list.index(class_name)\n",
    "\n",
    "                    bbox = obj.find('bndbox')\n",
    "                    xmin = int(bbox.find('xmin').text)\n",
    "                    ymin = int(bbox.find('ymin').text)\n",
    "                    xmax = int(bbox.find('xmax').text)\n",
    "                    ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "                    # Convert coordinates to YOLO format\n",
    "                    x_center = (xmin + xmax) / (2.0 * width)\n",
    "                    y_center = (ymin + ymax) / (2.0 * height)\n",
    "                    bbox_width = (xmax - xmin) / width\n",
    "                    bbox_height = (ymax - ymin) / height\n",
    "\n",
    "                    # Write the YOLO annotation\n",
    "                    file.write(f'{class_index} {x_center} {y_center} {bbox_width} {bbox_height}\\n')\n",
    "\n",
    "            print(f'Converted {filename} to YOLO format.')\n",
    "\n",
    "# Example usage\n",
    "pascal_dir = r\"C:\\Users\\LAPTOP WORLD\\Downloads\\no lbl\"\n",
    "output_dir = r\"C:\\Users\\LAPTOP WORLD\\Downloads\\no lbl\"\n",
    "class_list = ['cabincruiser', 'halfcab', 'PWC', 'kayak', 'open','human','PDF','registration_number']  # Modify with your own class list\n",
    "convert_pascalvoc_to_yolo(pascal_dir, output_dir, class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith('.jpg'):\n",
    "        if file.replace('jpg','txt') not in os.listdir(output_dir):\n",
    "            os.remove(os.path.join(output_dir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(r'data\\training\\labels'):\n",
    "    if file.endswith('.txt'):\n",
    "        if file.replace('txt','jpg') not in os.listdir(r\"data\\training\\images\"):\n",
    "            os.remove(os.path.join(r'data\\training\\labels',file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(r'data\\training\\images'):\n",
    "    if img in os.listdir(r'data\\validation\\images'):\n",
    "        os.remove(os.path.join(r'data\\training\\images',img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl in os.listdir(r'data\\training\\labels'):\n",
    "    if lbl.replace('txt','jpg') in os.listdir(r'data\\validation\\images'):\n",
    "        shutil.move(os.path.join(r'data\\training\\labels',lbl),os.path.join(r'data\\validation\\labels',lbl))\n",
    "for lbl in os.listdir(r'data\\validation\\labels'):\n",
    "    if lbl.replace('txt','jpg') in os.listdir(r'data\\training\\images'):\n",
    "        shutil.move(os.path.join(r'data\\validation\\labels',lbl),os.path.join(r'data\\training\\labels',lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great, now for my next project, try to write a readme file for this project's codes:\n",
    "Training_allImages_cbinNotOpen.ipynb:\n",
    "YOLOv5\n",
    "YOLOv5 is an object detection model that can detect multiple objects in an image. The code for training and testing the model is available on GitHub.\n",
    "\n",
    "Code\n",
    "To install the necessary dependencies and train the model, the following code can be used:\n",
    "\n",
    "Installing dependencies:\n",
    "# Clone the yolov5 repository using Git\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "\n",
    "# Install the necessary dependencies for the yolov5 repository using pip\n",
    "!pip install -U -r yolov5/requirements.txt \n",
    "\n",
    "# Change the working directory to the cloned yolov5 repository\n",
    "%cd yolov5\n",
    "Cloning into 'yolov5'...\n",
    "c:\\python\\projects\\Upwork-Projects\\Sara RCNN\\YOLO\\yolov5\\yolov5\n",
    "Training the model:\n",
    "# Train YOLOv5 object detection model with specified configuration and hyperparameters\n",
    "!python train.py --img 640 --batch 32 --epochs 100 --data data.yaml --cfg models/yolov5s.yaml --weights models/yolov5s.pt --name yolo_model --nosave --cache\n",
    "train: weights=models/yolov5s.pt, cfg=models/yolov5s.yaml, data=data.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=8, batch_size=8, imgsz=416, rect=False, resume=False, nosave=True, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=yolo_model, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
    "github: up to date with https://github.com/ultralytics/yolov5 \n",
    "Traceback (most recent call last):\n",
    "  File \"c:\\python\\projects\\Upwork-Projects\\Sara RCNN\\YOLO\\yolov5\\yolov5\\train.py\", line 642, in <module>\n",
    "    main(opt)\n",
    "  File \"c:\\python\\projects\\Upwork-Projects\\Sara RCNN\\YOLO\\yolov5\\yolov5\\train.py\", line 506, in main\n",
    "    check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\n",
    "  File \"c:\\python\\projects\\Upwork-Projects\\Sara RCNN\\YOLO\\yolov5\\yolov5\\utils\\general.py\", line 508, in check_file\n",
    "    assert len(files), f'File not found: {file}'  # assert file was found\n",
    "AssertionError: File not found: data.yaml\n",
    "Validating the model:\n",
    "To validate the trained model and test it on images, the following code can be used:\n",
    "\n",
    "!python yolov5/val.py --data yolov5/data.yaml --weights last.pt\n",
    "Detectening:\n",
    "# !python detect.py --source \"data\\validation\\images\\9a1b9061-0697F00000tQ1RYQA0-September-30-2021-15_50_20-AEST-1000.jpg\" --weights runs/train/yolo_model4/weights/last.pt --conf 0.5\n",
    "detect: weights=['yolov5/runs/train/yolo_model4/weights/last.pt'], source=data\\validation\\images\\9a1b9061-0697F00000tQ1RYQA0-September-30-2021-15_50_20-AEST-1000.jpg, data=yolov5\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5\\runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
    "git: 'RCNN\\YOLO\\yolov5' is not a git command. See 'git --help'.\n",
    "YOLOv5  2023-5-5 Python-3.9.4 torch-2.0.0+cpu CPU\n",
    "\n",
    "Fusing layers... \n",
    "YOLOv5s summary: 157 layers, 7064065 parameters, 0 gradients, 15.9 GFLOPs\n",
    "image 1/1 C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\YOLO\\data\\validation\\images\\9a1b9061-0697F00000tQ1RYQA0-September-30-2021-15_50_20-AEST-1000.jpg: 640x480 3 Humans, 2 PFD (lifejacket) s, 3 PWCs, 3 Registration Numbers, 2 Structures, 152.0ms\n",
    "Speed: 0.0ms pre-process, 152.0ms inference, 1.5ms NMS per image at shape (1, 3, 640, 640)\n",
    "Results saved to yolov5\\runs\\detect\\exp4\n",
    "import os   # Import the os module to execute shell commands\n",
    "import time # Import the time module to pause the execution of the script\n",
    "\n",
    "\n",
    "for img in os.listdir(r\"New_DataSet\\New_DataSet\"):\n",
    "    # Loop over all the images in the \"data/validation/images\" directory\n",
    "    # and store the filename in the variable \"img\"\n",
    "\n",
    "    os.popen(fr'python yolov5/detect.py --source \"New_DataSet\\New_DataSet\\{img}\" --weights last.pt --conf 0.1')\n",
    "    # Execute the shell command to run the YOLOv5 model on the current image.\n",
    "    # The \"--source\" argument specifies the input image file, \"--weights\" specifies\n",
    "    # the pre-trained weights file, and \"--conf\" specifies the minimum confidence\n",
    "    # threshold for the detected objects.\n",
    "\n",
    "    time.sleep(6)\n",
    "    # Pause the execution of the script for 6 seconds to allow time for the model\n",
    "    # to detect objects in the current image.\n",
    "\n",
    "    print(img, ' Done!')\n",
    "    # Print a message to i\n",
    "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.\n",
    "Old_DataSet Testing and validation Performance Matrix:\n",
    "Now we're going to test our model on the Validation dataset as it's data that the model has never seen before and also it got true labels that we can compare our model's predictions to.\n",
    "# Run validation script for YOLOv5\n",
    "# !python yolov5/val.py --data data.yaml --weights last.pt\n",
    "val: data=/content/drive/MyDrive/data.yaml, weights=['/content/yolov5/runs/train/yolo_model4/weights/last.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=../runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
    "requirements: /content/requirements.txt not found, check failed.\n",
    "YOLOv5 🚀 v7.0-162-gc3e4e94 Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
    "\n",
    "Fusing layers... \n",
    "YOLOv5s summary: 157 layers, 7064065 parameters, 0 gradients, 15.9 GFLOPs\n",
    "val: Scanning /content/drive/MyDrive/data/validation/labels.cache... 975 images, 0 backgrounds, 0 corrupt: 100% 975/975 [00:00<?, ?it/s]\n",
    "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 31/31 [00:27<00:00,  1.15it/s]\n",
    "                   all        975       4318      0.939      0.832      0.861      0.625\n",
    "         Cabin Cruiser        975        167      0.996      0.994      0.995      0.848\n",
    "           Canoe/Kayak        975         79      0.998      0.924      0.978      0.619\n",
    "            Commercial        975          6      0.818          1      0.995      0.899\n",
    "              Half Cab        975        398      0.972      0.967      0.991      0.831\n",
    "                 Human        975       1563      0.945      0.867      0.908      0.598\n",
    "                  Open        975        236      0.922      0.951      0.964      0.784\n",
    "                 Other        975          2          1          0     0.0717     0.0645\n",
    "     PFD (lifejacket)         975        563      0.922      0.773      0.827      0.447\n",
    "                   PWC        975        263      0.954      0.942      0.947      0.656\n",
    "   Registration Number        975        686      0.903      0.878      0.895       0.52\n",
    "             Structure        975        355      0.903      0.854      0.896      0.603\n",
    "Speed: 0.3ms pre-process, 5.0ms inference, 2.7ms NMS per image at shape (32, 3, 640, 640)\n",
    "Results saved to ../runs/val/exp2\n",
    "These results appear to be from an object detection model trained to detect various objects within images, with evaluation metrics reported for each class of object.\n",
    "\n",
    "The table lists the following information for each class:\n",
    "\n",
    "The number of images in the test set that contain instances of the object class.\n",
    "The total number of instances of the object class detected in the test set.\n",
    "The precision (P) and recall (R) values for the object class, which are commonly used evaluation metrics for object detection models. Precision measures the fraction of detected instances that are correct, while recall measures the fraction of true instances that were detected.\n",
    "The mAP50 and mAP50-95 values for the object class, which are the mean average precision over all possible intersection over union (IoU) thresholds from 0.5 to 0.95, with a step size of 0.05. The mAP50 is calculated using only IoU thresholds of 0.5, while mAP50-95 uses thresholds from 0.5 to 0.95. These metrics provide a measure of overall detection accuracy for the model.\n",
    "The \"all\" row in the table provides aggregated metrics over all object classes. It shows that the model achieved an overall precision of 0.939 and recall of 0.832, resulting in an overall mAP50 of 0.861 and mAP50-95 of 0.625.\n",
    "\n",
    "For each individual object class, the model achieved varying levels of performance. For example, the \"Cabin Cruiser\" class achieved high precision and recall values, resulting in high mAP50 and mAP50-95 values, indicating that the model was able to accurately detect instances of this object class. However, the \"Other\" class achieved perfect precision but zero recall, indicating that the model was able to correctly classify instances when they were present, but was not able to detect any instances in the test set.\n",
    "\n",
    "Overall, these results provide a snapshot of how well the object detection model performed in detecting different classes of objects in the test set.\n",
    "\n",
    "From the apove results we can see that our model's accuracy on each class was as follow\n",
    "Cabin Cruiser -> 99.5%\n",
    "Canoe/Kayak -> 97.8%\n",
    "Commercial -> 99.5%\n",
    "Half Cab -> 99.1%\n",
    "Human -> 90.8%\n",
    "Open -> 96.4%\n",
    "PFD (lifejacket) -> 82.7%\n",
    "PWC 94.7%\n",
    "Registration Number -> 89.5%\n",
    "Structure 89.6%\n",
    "Confusion Matrix:\n",
    "confusion_matrix\n",
    "\n",
    "Looking at this confusio matrix of our model we can see some details:\n",
    "\n",
    "our model predicted some Other as Cabin Cruiser class.\n",
    "our model predicted Commercials was right!\n",
    "our data hsd no Dinghies/Cats, Hire & Drive, House Boats, Kites, Rowings, SUPs, Ski Boats, Windsurfers or Yachts, so it would be better if we removed them from our classes as they may confuse our model.\n",
    "our model wasn't the best in predicting Huemans as it only predicted 82% from them true, but we can improve this by giving it more data with Humans in it to train on.\n",
    "our model predicted all Other class wrong, so I suggest we better remove it too from our classes.\n",
    "our model wasn't the best in predicting PFDs as it only predicted 77% from them true, and predicted 16% of them as Background, but we can improve this by giving it more data with PFDs in it to train on.\n",
    "our model predicted some Regestration numbers as Background class.\n",
    "PR_curve\n",
    "\n",
    "The precision-recall (PR) curve is a graphical representation of the performance of a binary classifier, plotting precision and recall for different thresholds.\n",
    "\n",
    "Precision is the proportion of true positive predictions out of all positive predictions, while recall is the proportion of true positive predictions out of all actual positive instances. In other words, precision measures the accuracy of the positive predictions, while recall measures the completeness of the positive predictions.\n",
    "\n",
    "The PR curve is useful when there is an imbalance in the distribution of positive and negative instances in the data. It allows us to evaluate the trade-off between precision and recall for different thresholds, and to select the threshold that maximizes the F1 score, which is the harmonic mean of precision and recall.\n",
    "\n",
    "A perfect classifier would achieve a precision of 1.0 and a recall of 1.0, resulting in a PR curve that passes through the point (1.0, 1.0). A classifier that predicts at random would result in a PR curve that follows the line y = x (i.e., the diagonal of the plot), with an area under the curve (AUC) of 0.5.\n",
    "\n",
    "The area under the PR curve (AUC-PR) provides a single number that summarizes the overall performance of the classifier, with a higher AUC-PR indicating better performance.\n",
    "\n",
    "And for me, this PR_curve is very good and proves that our model can be relied on in detecting most of our classes.\n",
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Initializing counter variable n to 0\n",
    "n = 0\n",
    "\n",
    "# Iterating through each directory in the specified path\n",
    "for d in os.listdir(r'yolov5\\runs\\detect'):\n",
    "    \n",
    "    # Iterating through each file in the directory\n",
    "    for i in os.listdir(os.path.join(r'yolov5\\runs\\detect',d)):\n",
    "        # Checking if the counter variable n is less than 10\n",
    "        if n < 10:\n",
    "                # Reading the image file using plt.imread() and displaying it using plt.imshow()\n",
    "            img = plt.imread(os.path.join(r'yolov5\\runs\\detect',d,i))\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "                # Incrementing the counter variable n by 1\n",
    "            n += 1\n",
    "        # Breaking the loop if the counter variable n reaches 10\n",
    "        else: \n",
    "            break\n",
    "val_batch2_pred\n",
    "\n",
    "New_DataSet_Testing_And_Validation:\n",
    "# import os\n",
    "\n",
    "# for f in os.listdir('images'):\n",
    "#     if ' - ' not in f:\n",
    "#         os.remove(os.path.join('images',f))\n",
    "import os   # Import the os module to execute shell commands\n",
    "import time # Import the time module to pause the execution of the script\n",
    "\n",
    "\n",
    "for folder in os.listdir(r\"images\"):\n",
    "    if not folder.endswith(\".jpg\"):\n",
    "        for img in os.listdir(os.path.join('images',folder)):\n",
    "\n",
    "            # Loop over all the images in the \"data/validation/images\" directory\n",
    "            # and store the filename in the variable \"img\"\n",
    "\n",
    "            os.popen(fr'python yolov5/detect_txt.py --source \"images\\{folder}\\{img}\" --weights last.pt --conf 0.5 --save-txt --save-conf')\n",
    "            # Execute the shell command to run the YOLOv5 model on the current image.\n",
    "            # The \"--source\" argument specifies the input image file, \"--weights\" specifies\n",
    "            # the pre-trained weights file, and \"--conf\" specifies the minimum confidence\n",
    "            # threshold for the detected objects.\n",
    "\n",
    "            time.sleep(4)\n",
    "            # Pause the execution of the script for 6 seconds to allow time for the model\n",
    "            # to detect objects in the current image.\n",
    "\n",
    "            print(img, ' Done!')\n",
    "            # Print a message to indicate that the\n",
    "# class_dict = {0: 'CabinCruiser', 1: 'HumanPowered', 4: 'HalfCab', 4: 'Open'}\n",
    "class_dict = {'CabinCruiser': 0,'Commercial':2, 'CanoeKayak': 1, 'Open': 4, 'HalfCab':4,'PWC':12}\n",
    "labelslst = [] # Initialize empty list to store labels.\n",
    "detectpath = r'yolov5/runs/detect' # Set path to directory containing experiment folders.\n",
    "for expfolder in os.listdir(detectpath): # Iterate over experiment folders.\n",
    "    for labelsfile in os.listdir(os.path.join(detectpath,expfolder,'labels')): # Iterate over label files in each experiment folder.\n",
    "        with open(os.path.join(detectpath,expfolder,'labels',labelsfile), 'r') as file: # Open each label file.\n",
    "            for line in file: # Iterate over lines in the label file.\n",
    "                if int(line.replace('\\n','').split(' ')[0]) in class_dict.values(): # Check if the line contains a class ID that is present in the class dictionary.\n",
    "                    classlst = line.replace('\\n','').split(' ') # Split the line into a list.\n",
    "                    classlst = [round(float(item),1) for item in classlst] # Convert each item in the list to a float and round to 1 decimal place.\n",
    "                    predclass = [name for name,number in class_dict.items() if number == int(line.replace('\\n','').split(' ')[0])][0].lower()  # Get the predicted class name from the class dictionary.\n",
    "                    realclass = labelsfile.split('-')[0].lower().replace('under5kn','').replace('over5kn','').strip() # Get the real class name from the label file name.\n",
    "                    classlst.insert(0,labelsfile.replace('.txt','.jpg')) # Add the image file name to the beginning of the list.\n",
    "                    classlst.insert(2, predclass) # Add the predicted class name to the list.\n",
    "                    classlst.insert(3, realclass) # Add the real class name to the list.\n",
    "                    labelslst.append(classlst) # Append the list to the label list.\n",
    "\n",
    "labelslst[0]\n",
    "['CabinCruiserUnder5Kn-0b3332b0-3c2e-4649-b5be-586066839aa3.jpg',\n",
    " 4.0,\n",
    " 'open',\n",
    " 'cabincruiser',\n",
    " 0.2,\n",
    " 0.3,\n",
    " 0.4,\n",
    " 0.2,\n",
    " 0.9]\n",
    "import pandas as pd  # Import the Pandas library\n",
    "\n",
    "# Create a DataFrame from a list called `labelslst`, with specific column names\n",
    "lablelsdf = pd.DataFrame(labelslst, columns=['image_name','pred_class_number','pred_class_name','real_class_name','bbox1','bbox2','bbox3','bbox4','confidence_score'])\n",
    "\n",
    "# Drop duplicate rows in the DataFrame based on specific columns\n",
    "lablelsdf.drop_duplicates(subset=['image_name','pred_class_number','bbox1','bbox2'],inplace=True)\n",
    "\n",
    "# Return the resulting DataFrame after removing duplicate rows\n",
    "lablelsdf\n",
    "image_name\tpred_class_number\tpred_class_name\treal_class_name\tbbox1\tbbox2\tbbox3\tbbox4\tconfidence_score\n",
    "0\tCabinCruiserUnder5Kn-3e7bd318-adea-4749-a5a8-4...\t4.0\topen\tcabincruiser\t0.1\t0.3\t0.2\t0.2\t0.9\n",
    "1\tCabinCruiserUnder5Kn-3f9b2d89-7e29-49db-bcb7-5...\t4.0\topen\tcabincruiser\t0.7\t0.4\t0.6\t0.3\t0.8\n",
    "2\tCabinCruiserUnder5Kn-42025ea3-5642-42d9-a8f6-5...\t4.0\topen\tcabincruiser\t0.6\t0.6\t0.8\t0.5\t0.8\n",
    "3\tCabinCruiserUnder5Kn-464afd78-62c0-41f6-b76a-7...\t4.0\topen\tcabincruiser\t0.6\t0.5\t0.2\t0.1\t0.9\n",
    "4\tCabinCruiserUnder5Kn-472b404c-cf3f-4cd4-86e3-3...\t4.0\topen\tcabincruiser\t0.7\t0.4\t0.6\t0.4\t0.5\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "442\tCabinCruiserUnder5Kn-3881fe46-2a2a-4468-bd8d-6...\t4.0\topen\tcabincruiser\t0.6\t0.4\t0.6\t0.3\t0.8\n",
    "443\tCabinCruiserUnder5Kn-3881fe46-2a2a-4468-bd8d-6...\t4.0\topen\tcabincruiser\t0.6\t0.6\t0.3\t0.3\t0.9\n",
    "444\tCabinCruiserUnder5Kn-39835a77-59c4-4570-9381-2...\t4.0\topen\tcabincruiser\t0.4\t0.2\t0.3\t0.2\t0.9\n",
    "445\tCabinCruiserUnder5Kn-39baf9ac-203d-4996-a506-8...\t4.0\topen\tcabincruiser\t0.4\t0.4\t0.4\t0.2\t0.9\n",
    "446\tCabinCruiserUnder5Kn-3c3eaae5-e4af-4e54-aacc-0...\t0.0\tcabincruiser\tcabincruiser\t0.7\t0.4\t0.5\t0.4\t0.9\n",
    "447 rows × 9 columns\n",
    "\n",
    "# group the dataframe by 'image_name', and select the row with the highest 'confidence_score'\n",
    "# for each group using the 'idxmax()' method\n",
    "lablelsdf_filtered = lablelsdf.loc[lablelsdf.groupby('image_name')['confidence_score'].idxmax()].reset_index(drop=True)\n",
    "lablelsdf_filtered.real_class_name[lablelsdf_filtered.real_class_name == 'halfcab'] = 'open'\n",
    "# return the filtered dataframe\n",
    "lablelsdf_filtered\n",
    "C:\\Users\\LAPTOP WORLD\\AppData\\Local\\Temp\\ipykernel_27512\\832828786.py:4: SettingWithCopyWarning: \n",
    "A value is trying to be set on a copy of a slice from a DataFrame\n",
    "\n",
    "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "  lablelsdf_filtered.real_class_name[lablelsdf_filtered.real_class_name == 'halfcab'] = 'open'\n",
    "image_name\tpred_class_number\tpred_class_name\treal_class_name\tbbox1\tbbox2\tbbox3\tbbox4\tconfidence_score\n",
    "0\tCabinCruiserOver5Kn-43bcd1e7-ab32-4723-bc95-d4...\t4.0\topen\tcabincruiser\t0.1\t0.8\t0.3\t0.5\t0.9\n",
    "1\tCabinCruiserOver5Kn-7abeee81-5ec7-43cf-ac74-b8...\t4.0\topen\tcabincruiser\t0.5\t0.3\t0.3\t0.2\t0.9\n",
    "2\tCabinCruiserOver5Kn-881e4b3a-6a54-4db0-b6df-0a...\t4.0\topen\tcabincruiser\t0.5\t0.3\t0.6\t0.4\t0.9\n",
    "3\tCabinCruiserOver5Kn-8b589e36-2a1a-45ac-880e-d4...\t4.0\topen\tcabincruiser\t0.7\t0.3\t0.5\t0.2\t0.9\n",
    "4\tCabinCruiserOver5Kn-9628196e-cd7c-49b2-aa93-e4...\t4.0\topen\tcabincruiser\t0.8\t0.2\t0.4\t0.3\t0.9\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "420\tOpenUnder5kn-994e20ad-cf7e-47f9-bb24-d1cf59fdc...\t4.0\topen\topen\t0.5\t0.5\t0.6\t0.3\t0.6\n",
    "421\tOpenUnder5kn-aa8b06fe-5b08-4475-beae-396c48c85...\t4.0\topen\topen\t0.3\t0.3\t0.5\t0.2\t0.9\n",
    "422\tOpenUnder5kn-b0e278c0-a592-4c4d-bd98-77ca11e1e...\t4.0\topen\topen\t0.4\t0.5\t0.6\t0.3\t0.9\n",
    "423\tOpenUnder5kn-c5558165-441c-4ea8-ab91-7c60a9150...\t4.0\topen\topen\t0.3\t0.6\t0.7\t0.3\t0.9\n",
    "424\tOpenUnder5kn-e99aa0f7-9675-4359-9780-1d56ce724...\t4.0\topen\topen\t0.3\t0.4\t0.6\t0.3\t0.9\n",
    "425 rows × 9 columns\n",
    "\n",
    "lablelsdf_filtered.to_csv('ImageLabels.csv',index=False) # saving to a csv.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Calculate accuracy, precision, and recall scores\n",
    "acc = accuracy_score(lablelsdf_filtered.real_class_name, lablelsdf_filtered.pred_class_name) # Calculates the accuracy score of the model\n",
    "pre = precision_score(lablelsdf_filtered.real_class_name, lablelsdf_filtered.pred_class_name, average='macro') # Calculates the macro-averaged precision score of the model\n",
    "rec = recall_score(lablelsdf_filtered.real_class_name, lablelsdf_filtered.pred_class_name, average='macro') # Calculates the macro-averaged recall score of the model\n",
    "\n",
    "print(f\"\"\"\n",
    "accuracy = {round(acc*100,2)}%\n",
    "precision = {round(pre*100,2)}%\n",
    "recall = {round(rec*100,2)}%\n",
    "\"\"\")\n",
    "accuracy = 60.71%\n",
    "precision = 59.19%\n",
    "recall = 45.76%\n",
    "\n",
    "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true, y_pred = lablelsdf_filtered.real_class_name, lablelsdf_filtered.pred_class_name\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm, cmap='RdYlGn')\n",
    "\n",
    "# Add labels and ticks to the plot\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_xticks(np.arange(len(np.unique(y_true))))\n",
    "ax.set_yticks(np.arange(len(np.unique(y_true))))\n",
    "ax.set_xticklabels(np.unique(y_true))\n",
    "ax.set_yticklabels(np.unique(y_true))\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add text to the plot\n",
    "for i in range(len(np.unique(y_true))):\n",
    "    for j in range(len(np.unique(y_true))):\n",
    "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "From this confusion matrix we can see that our model predicted all of the new data's cabincruisers and Humanpowereds as mostly halfcaps, it was good in predecting the Opens, I guess we need to look at our data and see if it's well labeled.\n",
    "\n",
    "This code reads the images in the data/validation/images directory one by one, runs the YOLOv5 object detection model on each image using the detect.py script, and waits for 6 seconds before moving on to the next image. The last.pt file contains the pre-trained weights of the YOLOv5 model. The detected objects are saved in the runs/detect directory.\n",
    "\n",
    "You'll find the predections in this path:\n",
    "yolov5\\runs\\detect\\\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set initial counter to zero\n",
    "n = 0\n",
    "# Loop through directories in the detect folder\n",
    "for d in os.listdir(r'yolov5\\runs\\detect'):\n",
    "    # Loop through images in each directory\n",
    "     for i in os.listdir(os.path.join(r'yolov5\\runs\\detect',d)):\n",
    "     \n",
    "          # Check if image is a jpeg and counter is less than 10\n",
    "          if n < 10 and i.endswith('.jpg'):\n",
    "                      # Read image using matplotlib and display\n",
    "               img = plt.imread(os.path.join(r'yolov5\\runs\\detect',d,i))\n",
    "               plt.imshow(img)\n",
    "               plt.show()\n",
    "                      # Increase counter\n",
    "               n += 1\n",
    "            # Break out of inner loop if counter is greater than or equal to 10\n",
    "          else:\n",
    "               break\n",
    "import csv\n",
    "\n",
    "class_dict = {0: 'CabinCruiser', 1: 'HumanPowered', 2: 'Commercial', 4: 'Open', 13: 'HalfCab'} # dictionary mapping class numbers to object names\n",
    "\n",
    "for expfolder in os.listdir(detectpath): # Iterate over experiment folders.\n",
    "    for labelsfile in os.listdir(os.path.join(detectpath,expfolder,'labels')):\n",
    "        input_file = os.path.join(os.path.join(detectpath,expfolder,'labels'),labelsfile)\n",
    "        output_file = os.path.join('CSVs',labelsfile.replace('.txt','.csv'))\n",
    "\n",
    "        with open(input_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        with open(output_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['object name', 'conf']) # header row\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                class_num = int(parts[0])\n",
    "                if class_num in class_dict:\n",
    "                    object_name = class_dict[class_num]\n",
    "                    conf = parts[-1]\n",
    "                    writer.writerow([object_name, conf])\n",
    "\n",
    "datamanipulation.ipynb\n",
    "# Spliting the data:\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "ii = 0\n",
    "for d in [r\"data_\\training\",r\"data_\\training\"]:\n",
    "    for dd in os.listdir(d):\n",
    "        if 'images' in dd:\n",
    "            for img in os.listdir(os.path.join(d,dd)):\n",
    "                # Generate a random float between 0 and 1\n",
    "                random_number = random.random()\n",
    "                if random_number < test_size:\n",
    "                    shutil.copy(os.path.join(d,dd,img),os.path.join('data','validation','images',img))\n",
    "                    shutil.copy(os.path.join(d,dd.replace('images','labels'),img.replace('.jpg','.txt')),os.path.join('data','validation','labels',img.replace('.jpg','.txt')))\n",
    "                else:\n",
    "                    shutil.copy(os.path.join(d,dd,img),os.path.join('data','training','images',img))\n",
    "                    shutil.copy(os.path.join(d,dd.replace('images','labels'),img.replace('.jpg','.txt')),os.path.join('data','training','labels',img.replace('.jpg','.txt')))\n",
    "for dir_ in [r\"data\\training\\images\",r\"data\\validation\\images\"]:\n",
    "    for img in os.listdir(dir_):\n",
    "        if 'over' in img.lower() and dir_ == r\"data\\training\\images\":\n",
    "            shutil.move(os.path.join(dir_,img),os.path.join(r\"data\\validation\\images\",img))\n",
    "        elif 'under' in img.lower() and dir_ == r\"data\\validation\\images\":\n",
    "            shutil.move(os.path.join(dir_,img),os.path.join(r\"data\\training\\images\",img))\n",
    "for dir_ in [r\"data\\training\\labels\",r\"data\\validation\\labels\"]:\n",
    "    for lbl in os.listdir(dir_):\n",
    "        if 'over' in lbl.lower() and dir_ == r\"data\\training\\labels\":\n",
    "            shutil.move(os.path.join(dir_,lbl),os.path.join(r\"data\\validation\\labels\",lbl))\n",
    "        elif 'under' in lbl.lower() and dir_ == r\"data\\validation\\labels\":\n",
    "            shutil.move(os.path.join(dir_,lbl),os.path.join(r\"data\\training\\labels\",lbl))\n",
    "for img in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\"):\n",
    "    if img.replace('.jpg','.txt') not in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\labels\"):\n",
    "        os.remove(os.path.join(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\",img))\n",
    "for img in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\"):\n",
    "    val_img = '-'+ img\n",
    "    if val_img in os.listdir(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\validation\\images\"):\n",
    "        os.remove(os.path.join(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\images\",img))\n",
    "        os.remove(os.path.join(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\data\\training\\labels\",img.replace('jpg','txt')))\n",
    "labelsdir = r'data/validation/labels/'\n",
    "\n",
    "for labelfile in os.listdir(labelsdir):\n",
    "    if 'over'in labelfile.lower() or 'under' in labelfile.lower() or 'human' in labelfile.lower():\n",
    "        # Open the file in read mode and read its contents\n",
    "        with open(os.path.join(labelsdir,labelfile), 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Modify the desired text\n",
    "        modified_text = text.replace('4 ', '9 ')\n",
    "        modified_text = modified_text.replace('1 ', '4 ')\n",
    "        modified_text = modified_text.replace('2 ', '12 ')\n",
    "        modified_text = modified_text.replace('3 ', '1 ')\n",
    "        modified_text = modified_text.replace('7 ', '13 ')\n",
    "        modified_text = modified_text.replace('5 ', '7 ')\n",
    "        modified_text = modified_text.replace('6 ', '11 ')\n",
    "\n",
    "\n",
    "        # Open the file in write mode and overwrite the original contents\n",
    "        with open(os.path.join(labelsdir,labelfile),'w') as file:\n",
    "            file.write(modified_text)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "def convert_pascalvoc_to_yolo(pascal_dir, output_dir, class_list):\n",
    "    for filename in os.listdir(pascal_dir):\n",
    "        if filename.endswith('.xml'):\n",
    "            # Read the XML annotation file\n",
    "            xml_path = os.path.join(pascal_dir, filename)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract image size\n",
    "            width = int(root.find('size/width').text)\n",
    "            height = int(root.find('size/height').text)\n",
    "\n",
    "            # Create the YOLO annotation file\n",
    "            yolo_path = os.path.join(output_dir, filename.replace('.xml', '.txt'))\n",
    "            with open(yolo_path, 'w') as file:\n",
    "                for obj in root.findall('object'):\n",
    "                    # Extract object class and bounding box coordinates\n",
    "                    class_name = obj.find('name').text\n",
    "                    if class_name == 'registration_numver':\n",
    "                        class_name = 'registration_number'\n",
    "                    class_index = class_list.index(class_name)\n",
    "\n",
    "                    bbox = obj.find('bndbox')\n",
    "                    xmin = int(bbox.find('xmin').text)\n",
    "                    ymin = int(bbox.find('ymin').text)\n",
    "                    xmax = int(bbox.find('xmax').text)\n",
    "                    ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "                    # Convert coordinates to YOLO format\n",
    "                    x_center = (xmin + xmax) / (2.0 * width)\n",
    "                    y_center = (ymin + ymax) / (2.0 * height)\n",
    "                    bbox_width = (xmax - xmin) / width\n",
    "                    bbox_height = (ymax - ymin) / height\n",
    "\n",
    "                    # Write the YOLO annotation\n",
    "                    file.write(f'{class_index} {x_center} {y_center} {bbox_width} {bbox_height}\\n')\n",
    "\n",
    "            print(f'Converted {filename} to YOLO format.')\n",
    "\n",
    "# Example usage\n",
    "pascal_dir = r\"C:\\Users\\LAPTOP WORLD\\Downloads\\no lbl\"\n",
    "output_dir = r\"C:\\Users\\LAPTOP WORLD\\Downloads\\no lbl\"\n",
    "class_list = ['cabincruiser', 'halfcab', 'PWC', 'kayak', 'open','human','PDF','registration_number']  # Modify with your own class list\n",
    "convert_pascalvoc_to_yolo(pascal_dir, output_dir, class_list)\n",
    "\n",
    "\n",
    "app.py:\n",
    "from flask import Flask, render_template, request\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from IPython.display import Image  # for displaying images\n",
    "\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "yolo_model = torch.hub.load(r\"C:\\Users\\LAPTOP WORLD\\Downloads\\best (1).pt\",'custom', path='best.pt',force_reload=True,source='local', pretrained =False)\n",
    "\n",
    "# Set the device to use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "yolo_model.to(device).eval()\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "# yolo_model = torch.hub.load('.', 'custom', path=r\"C:\\Users\\LAPTOP WORLD\\Downloads\\best (1).pt\", source='local') \n",
    "\n",
    "# Set the device to use\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# yolo_model.to(device).eval()\n",
    "# Define the image transformation\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if 'image' not in request.files:\n",
    "        return \"No image uploaded.\"\n",
    "\n",
    "    image_file = request.files['image']\n",
    "    image = Image.open(image_file)\n",
    "\n",
    "    # Apply the transformation\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        predictions = yolo_model(image_tensor)\n",
    "\n",
    "        # Convert the PyTorch tensor to a PIL Image\n",
    "        output_image = transform.ToPILImage()(image_tensor.squeeze(0))\n",
    "\n",
    "        # Save or display the output image\n",
    "        output_image.save('detected_'+image_file)\n",
    "\n",
    "    # Process the predictions\n",
    "    # (You need to adapt this part based on your model's output format)\n",
    "\n",
    "    return \"Vessel detected!\"\n",
    "\n",
    "if __name__ == '_main_':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
